\input{configuration}

% Custom packages
\usepackage{xcolor}
\definecolor{solarizedBase03}{HTML}{FFFFFF}
\definecolor{solarizedBase02}{HTML}{000000}
\definecolor{solarizedBase00}{HTML}{657B83}
\definecolor{solarizedBase0}{HTML}{000000}
\definecolor{solarizedBase1}{HTML}{93A1A1}
\definecolor{solarizedBase3}{HTML}{FDF6E3}


\usepackage[listings]{tcolorbox}
\newtcbinputlisting{\codelisting}[2][]{
    listing file={#2},
    colback=solarizedBase03,
    colframe=solarizedBase02,
    colupper=solarizedBase0,
    fonttitle=\bfseries\color{solarizedBase1},
    listing options={basicstyle=\ttfamily\footnotesize},
    listing only,
    #1
}

\title{Lecture 21 --- GPU Programming }

\author{Patrick Lam and Jeff Zarnett \\ \small \texttt{patrick.lam@uwaterloo.ca}, \texttt{jzarnett@uwaterloo.ca}}
\institute{Department of Electrical and Computer Engineering \\
  University of Waterloo}
\date{\today}


\begin{document}

\begin{frame}
  \titlepage

 \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Introduction}


     \structure{CUDA}: coding on a heterogeneous architecture.
        No longer just programming the CPU; will also leverage the GPU.
        
      CUDA = Compute Unified Device Architecture.\\
      Usable on NVIDIA GPUs.
      
    \begin{center}
	\includegraphics[width=0.5\textwidth]{images/gpu.jpg}
	\end{center}
	
        It was hard to buy GPUs for a while. Probably it's better now.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{SIMT}



  Another term you may see vendors using:

  \begin{itemize}
    \item {\bf S}ingle {\bf I}nstruction, {\bf M}ultiple {\bf T}hreads.
    \item Runs on a vector of data.
    \item Similar to SIMD instructions (e.g. SSE).\\
     However, the vector is spread out over the GPU.
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Other Heterogeneous Programming Examples}


\begin{itemize}
\item PlayStation 3 Cell\\
\item OpenCL
\end{itemize}


See \url{https://www.youtube.com/watch?v=zW3XawAsaeU} for details on why it was hard to program for the PS3!

[PS4: back to a regular CPU/GPU system, albeit on one chip.]


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{(PS3) Cell Overview}


  Cell consists of:
  \begin{itemize}
    \item a PowerPC core; and
    \item 8 SIMD co-processors.
  \end{itemize}


  \begin{center}
    \includegraphics[scale=0.5]{images/cell}
  \end{center}
  \hfill (from the Linux Cell documentation)

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{CUDA Overview}
  

     Compute Unified Device Architecture:\\
     NVIDIA's architecture for processing on GPUs.\\[1em]

     ``C for CUDA'' predates OpenCL,\\
     NVIDIA supports it first and foremost.
      \begin{itemize}
        \item May be faster than OpenCL on NVIDIA hardware.
        \item API allows you to use (most) C++ features in CUDA; \\
          OpenCL has more restrictions.
      \end{itemize}

     OpenCL doesn't seem super popular these days.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{GPU Programming Model}



  The abstract model is simple:

  \begin{itemize}
    \item Write the code for the parallel computation ({\it kernel}) \\
      \qquad separately from main code.
    \item Transfer the data to the GPU co-processor \\
      \qquad (or execute it on the CPU).
    \item Wait \ldots
    \item Transfer the results back.
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Drive or Fly?}

It makes sense to hand it over to the GPU because there are a lot of cores.

\texttt{ecetesla2}: 4 core 3.6 GHz CPU; 1920 CUDA cores that run at about 1.8 GHz. 

So, half the speed, but 480 times the workers. 

\begin{center}
	\includegraphics[width=0.7\textwidth]{images/units.jpg}
\end{center}

\end{frame}


\begin{frame}
\frametitle{Drive or Fly?}

There's significant runtime overhead but the GPU is fast once started.

This is like driving vs flying. 

\begin{center}
	\includegraphics[width=0.45\textwidth]{images/ott-mtl.png}
\end{center}

\begin{center}
	\includegraphics[width=0.6\textwidth]{images/waterloo-sfo.png}
\end{center}


\end{frame}


\begin{frame}
\frametitle{CUDA Parallelism}

CUDA includes both task parallelism and data parallelism, as we've
discussed earlier in this course.

\emph{Data parallelism} is the central feature. 

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Data Parallelism}


  \begin{itemize}
    \item Key idea: evaluate a function (or {\it kernel}) \\
     \qquad over a set of points (data).\\

\begin{center}
\begin{tikzpicture}
\foreach \x in {0,0.25,...,2}
  \foreach \y in {0,0.25,...,2}
    \draw (\x, \y) circle (0.25mm);
\end{tikzpicture}
\end{center}

      Another example of data parallelism.\\[1em]
    \item Another name for the set of points: {\it index space}.
    \item Each point corresponds to a {\bf work-item}.
  \end{itemize}~\\[1em]

  Note: CUDA also supports {\it task parallelism} (using different kernels),
  but we won't focus on that in this course.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Work-Items}


    \structure{Work-item}: the fundamental unit of work in CUDA.
    
 These work-items live on an $n$-dimensional grid (ND-Range).
    
    \begin{center}
	\includegraphics[width=0.2\textwidth]{images/peon.jpeg}\\
	\hfill ``Ready to work!''
	\end{center}
    
\end{frame}
    
\begin{frame}
\frametitle{Work-Items}
    
You get your choice about block size. 

Usually, we say let the system decide. 

However, for some computations it's better to specify.

If you do, however, you want to make best use of the hardware and use a multiple of the size of the \textit{warp}.


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Shared Memory}


  There are many different types of memory available to you:


\begin{itemize}
\item private
\item local
\item global
\item constant
\item texture
\end{itemize}

Choosing what kind of memory to use is important!

There is also host memory (normal memory); usually contains app data.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile]
\frametitle{A kernel of my very own...}

Here's the \CPP~ code we start with:

\begin{lstlisting}[language=C++]
void vector_add(int n, const float *a, const float *b, float *c) {
  for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
}
\end{lstlisting}


The same code looks like this as a kernel:
\begin{lstlisting}[language=C++]
__global__ void vector_add(float* A, float* B, float* C)
{
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}
\end{lstlisting}

The loop became implicit!

\end{frame}


\begin{frame}
\frametitle{Going Backwards?}

You can write kernels in a variant of \CPP.


A large number of features of more recent versions of the language are supported.

How much will vary based on your release of the kernel compiler (\text{nvcc}).

It's unlikely that we'll be doing too many things that are exotic enough to be forbidden by the compiler.

\end{frame}


\begin{frame}
\frametitle{The Compiled Kernel}

The kernel is compiled into PTX---Parallel Thread eXecution---instructions.

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/hieroglyphics.jpg}
\end{center}

Can we write our own PTX? Yes. Will we? No.

Use the \texttt{nvcc} compiler.

Recompile if you change machines!

\end{frame}


\begin{frame}
\frametitle{Running \texttt{nvcc}}

The \texttt{nvcc} compiler has specific requirements for the underlying compiler it relies upon. 

For example, at some times on some \texttt{ecetesla} machines, \texttt{nvcc} fails without specifying \texttt{gcc} version 6 instead of the default. 

With the command
\vspace*{-4em}
\begin{center}
\texttt{nvcc --compiler-bindir /usr/bin/gcc-6 -ptx nbody.cu}
\end{center}
\vspace*{-2em}
it will compile.

We think it should just work currently.


\end{frame}


\begin{frame}
\frametitle{Danger! Danger!}

\begin{center}
	\includegraphics[width=0.3\textwidth]{images/unsafe.png}
\end{center}

The kernel has none of the safety guarantees that we normally get in Rust! 

We can easily read off the end of an array, forget to initialize a variable, allocate memory that we forget to deallocate...

So, just because it compiles, doesn't mean it will work.

\end{frame}


\begin{frame}[fragile]
\frametitle{I broke it...}

Here's something I got in testing:
{\tiny
\begin{verbatim}
thread 'main' panicked at 'Failed to deallocate CUDA Device memory.: IllegalAddress', 
/home/jzarnett/.cargo/registry/src/github.com-1ecc6299db9ec823/rustacuda-0.1.2/src/memory/device/device_buffer.rs:259:32
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
thread 'main' panicked at 'Failed to deallocate CUDA Device memory.: IllegalAddress', 
/home/jzarnett/.cargo/registry/src/github.com-1ecc6299db9ec823/rustacuda-0.1.2/src/memory/device/device_buffer.rs:259:32
stack backtrace:
\end{verbatim}
}

\end{frame}


\begin{frame}
\frametitle{N-Body Problem Kernel}

Let's take a look at a kernel for the N-Body problem!

We'll also use that when we examine how to launch a CUDA program.

\end{frame}


\begin{frame}
\frametitle{Practice Makes Perfect}
Let's think about how to write the kernel well.

We need to decide what sort of work belongs in the kernel to begin with.

Loops good, as long as they're not sequential (some coordination is okay).

\end{frame}


\begin{frame}
\frametitle{Two-Dimensions?}

What if we wanted to make the N-Body kernel a two-dimensional problem? 

We could instead treat each pair of points $(i, j)$ as a point in the space, making it a two-dimensional problem. 

Then you could think of it as a matrix rather than an array and provide it to CUDA like that. This might increase the parallelism! 


\end{frame}


\begin{frame}
\frametitle{Sounds Good, Doesn't Work}


\begin{center}
	\includegraphics[width=0.5\textwidth]{images/soundsgood.jpg}
\end{center}

The calculation of body-body interaction for just one pair of points is a very small amount of work, tiny even. 

Having one work-item for each such calculation means there's a lot of overhead to complete the calculation and it doesn't make sense. 

But if the calculation of the interaction were more complex, then this transformation might be an improvement!

\end{frame}

\begin{frame}
\frametitle{MORE DIMENSIONS!}

You can have a three-dimensional problem, but no more.

If you want something more than a three-dimensional, you have to have some loops in your code.

The limit to three dimensions is probably because graphics cards were designed for rendering images in 3-D so it seems logical?

\end{frame}


\begin{frame}
\frametitle{Go the other way?}

Of course, you can sometimes flatten the dimensions of your problem a bit. 

A rectangular array in C/\CPP~is really stored as a linear array and the \texttt{[x][y]} is just notational convenience so you could easily just treat it as a linear array.

\end{frame}


\begin{frame}
\frametitle{Brute Force a Password?}

Consider something like brute-forcing a password of 6 characters (easy, but just for the purpose of an example).

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/hackerman2.png}
\end{center}

A one-dimensional approach: generate all possibilities in host code.

Or, generate partial possibilities on host, complete them in kernel code.


\end{frame}



\begin{frame}
\frametitle{Branches}

The hardware will execute \emph{all} branches that any thread in a warp
      executes---can be slow!\\[1em]


\begin{center}
	\includegraphics[width=0.5\textwidth]{images/whynotboth.jpg}
\end{center}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Branches in kernels}

    \begin{lstlisting}
__global__ void contains_branch(float *a, float *b) {
    if (condition()) {
        a[blockIdx.x] += 5.0;
    } else {
        b[blockIdx.x] += 5.0;
    }
}
\end{lstlisting}

    
    In other words: an {\tt if} statement will cause each thread to execute
      both branches; we keep only the result of the taken branch.\\[1em]


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Loops in kernels}


    \begin{lstlisting}
__device__ void foo(int *p1, int *p2) {
  for (int i = 0; i < 12; ++i) {
    p1[i] += p2[i]*2;
  }
}
    \end{lstlisting}


    A loop will cause the workgroup to wait for the maximum number of
      iterations of the loop in any work-item.\\[1em]

The compiler will try to unroll loops if it can. (Here, yes, 12 is fine.)

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Atomics}

Race conditions can still occur. 

This means that if you want to, say, concurrently add to the same location, you need to use atomic functions. 

The atomic operations are usable on the standard primitive types.

There are the usual operations: add, sub, min, inc, dec, compare-and-swap, bitwise.

\end{frame}


\begin{frame}[fragile]
\frametitle{Atomic Example}

\begin{lstlisting}[language=C++]
__device__ double atomicAdd(double* address, double val) {
    unsigned long long int* address_as_ull =
                              (unsigned long long int*)address;
    unsigned long long int old = *address_as_ull, assumed;

    do {
        assumed = old;
        old = atomicCAS(address_as_ull, assumed,
                        __double_as_longlong(val +
                               __longlong_as_double(assumed)));

    // Note: uses integer comparison to avoid hang in case of NaN (since NaN != NaN)
    } while (assumed != old);

    return __longlong_as_double(old);
}
\end{lstlisting}

\end{frame}




\begin{frame}
\frametitle{Launch?}

\begin{center}
	\includegraphics[width=0.3\textwidth]{images/launch.jpg}
\end{center}

So far, all we have covered is the theory and then how to write a kernel. 

To make use of it, we'll have to look at the host code. That's our next topic. 

\end{frame}

\end{document}

