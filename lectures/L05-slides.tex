\input{configuration}

\title{Lecture 5 --- Asynchronous I/O}

\author{Patrick Lam \& Jeff Zarnett \\ \small \texttt{patrick.lam@uwaterloo.ca, jzarnett@uwaterloo.ca}}
\institute{Department of Electrical and Computer Engineering \\
  University of Waterloo}
\date{\today}


\begin{document}

\begin{frame}
  \titlepage

 \end{frame}


\begin{frame}
\frametitle{Dreams of Neo-Tokyo Tonight}

\begin{center}
  \fbox{\includegraphics[width=\textwidth]{images/L05-tokio.png}}
\end{center}

\url{https://www.youtube.com/watch?v=Wu5TDEpAqwQ}

\end{frame}


\begin{frame}[fragile]
\frametitle{Why Non-Blocking I/O?}

\begin{lstlisting}[language=Rust]
fn main() -> io::Result<()> {
  let mut file = File::open("hello.txt")?;
  let mut s = String::new();
  file.read_to_string(&mut s)?;
  Ok(()) 
}
\end{lstlisting}


(The ? operator is an alternative to
\texttt{try!} and \texttt{unwrap}.)


The problem is that the {\tt read} call will \alert{block}.

You don't get to use the CPU cycles while waiting.
\end{frame}


\begin{frame}
\frametitle{Threads Insteads?}

Threads can be fine if you have some other code running to do work.


But maybe you would rather not use threads. Why not?

\begin{itemize}
\item potential race conditions;
\item overhead due to per-thread stacks; or
\item limitations due to maximum numbers of threads.
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Non-Blocking I/O}

We're going to focus on low-level I/O from sockets in this part of the lecture,
using the \texttt{mio} library from \texttt{tokio}. 

\begin{center}
	\includegraphics[width=\textwidth]{images/neotokyo.jpg}
\end{center}

(Sadly, not Neo-Tokyo. This is from \textit{Akira}.)

\end{frame}

\begin{frame}
\frametitle{Gratuitous travel shot}

\begin{center}
	\includegraphics[width=.95\textwidth]{images/08257_shinjuku.jpg}
\end{center}
photo credit: Patrick Lam

\end{frame}

\begin{frame}
\frametitle{Non-Blocking I/O}




Async file I/O is
also possible via \texttt{tokio::fs} and the ideas will carry over.


One might often want to wrap the low-level I/O using higher-level
abstractions, and the larger project \texttt{tokio.rs} is one way of
doing that.


There are two ways to find out whether I/O is ready to be queried:\\
\quad (1) Polling and (2) Interrupts.

\texttt{mio} uses polling.

\end{frame}


\begin{frame}[fragile]
\frametitle{Using \texttt{mio}}

The key idea is to give {\tt mio} a bunch of event sources and
wait for events to happen. 

\begin{itemize}
       \item create a \texttt{Poll} instance;
       \item populate it with event sources e.g. \texttt{TCPListener}s; and,
       \item wait for events in an event loop ({\tt Poll::poll()}).
\end{itemize}
     
\begin{lstlisting}[language=Rust]
let poll = Poll::new()?;
let events = Events::with_capacity(128);
\end{lstlisting}

We're going to proactively create \texttt{events}; this data structure is used by
\texttt{Poll::poll} to stash the relevant \texttt{Event} objects.

\end{frame}


\begin{frame}[fragile]
\frametitle{Name a more iconic duo}

Populating the {\tt Poll} instance: ``registering event source''. 

This is a socket or file descriptor.

\begin{lstlisting}[language=Rust]
let mut listener = TcpListener::bind(address)?;
const SERVER: Token = Token(0);
poll.registry().register(&mut listener, SERVER, Interest::READABLE)?;
\end{lstlisting}


You're telling it to check for 
when the \texttt{listener} indicates that something is available to read.

\end{frame}


\begin{frame}[fragile]
\frametitle{Waiting for Events}

We're ready to wait for events on any registered listener.
    \begin{lstlisting}[language=Rust]
loop {
    poll.poll(&mut events, Some(Duration::from_millis(100)))?;

    for event in events.iter() {
        match event.token() {
            SERVER => loop {
                match listener.accept() {
                    Ok((connection, address)) => {
                        println!("Got a connection from: {}", address);
                    },
                    Err(ref err) if would_block(err) => break,
                    Err(err) => return Err(err),
                }
            }
        }
    }
}

fn would_block(err: &io::Error) -> bool {
    err.kind() == io::ErrorKind::WouldBlock
}    \end{lstlisting}

\texttt{poll.poll} will populate \texttt{events}, and
waits for at most 100 ms.

\end{frame}


\begin{frame}[fragile]
\frametitle{Network Programming}
If all you want to do is request a web page in Rust, use
the \texttt{reqwest} library.

\begin{lstlisting}[language=Rust]
let body = reqwest::get("https://www.rust-lang.org")
    .await?
    .text()
    .await?;

println!("body = {:?}", body);
\end{lstlisting}

(If you are doing multiple requests,
you should create your own \texttt{Client} and \texttt{get} from it instead of
\texttt{reqwest::get}).

\end{frame}


\begin{frame}
\frametitle{Back to the Futures}

\begin{center}
	\includegraphics[width=\textwidth]{images/bttf.jpg}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Back to the Futures}

The use of \texttt{await} is a bit tricky.

The \texttt{get} function returns a \emph{future}. 

What's that?  

It's
an object that will, at some point in the future, return a second
object.

\end{frame}


\begin{frame}
\frametitle{Plug-In Executors}

There are many possible definitions of \texttt{async/await}, and the appropriate one depends on your context. 

Rust allows you to specify a runtime
which defines the meaning of \texttt{async/await} for your program.

The simplest \texttt{await} just blocks and waits on the current
thread for the result to be ready. 

\end{frame}


\begin{frame}[fragile]
\frametitle{I'll Just Wait}
\begin{center}
	\includegraphics[width=0.3\textwidth]{images/ill-just-wait.jpg}
\end{center}


\begin{lstlisting}[language=Rust]
use futures::executor::block_on;

async fn hello_world() {
  println!("hello");
}

fn main() {
  let future = hello_world();
  block_on(future);
}
\end{lstlisting}

\end{frame}

\begin{frame}[fragile]
\frametitle{More Options}

\texttt{tokio} includes a more sophisticated executor as well. 

When there are multiple active \texttt{await}s, \texttt{tokio} can multiplex them onto different threads. 

You can specify the \texttt{tokio} executor (or others) with a
tag above {\tt main()} and by declaring \texttt{main()} to be \texttt{async}.

\begin{lstlisting}[language=Rust]
#[tokio::main]
async fn main() {
    // do async stuff
}
\end{lstlisting}

\end{frame}

\begin{frame}
\frametitle{Using libcurl}

libcurl is a C library for transferring files. 

First we'll start with the easy interface. 

This is a synchronous interface
that uses callbacks. 

\end{frame}


\begin{frame}[fragile]
\frametitle{Your HTML. Give it to me.}

\begin{lstlisting}[language=Rust]
use std::io::{stdout, Write};

use curl::easy::Easy;

// Write the contents of rust-lang.org to stdout
let mut easy = Easy::new();
easy.url("https://www.rust-lang.org/").unwrap();
easy.write_function(|data| { // callback function
    stdout().write_all(data).unwrap();
    Ok(data.len())
}).unwrap();
easy.perform().unwrap();
\end{lstlisting}

Note that we provide a lambda as a callback function. 

This lambda is to be invoked
when the library receives data from the network (i.e. \texttt{write\_function()}).

\end{frame}


\begin{frame}
\frametitle{Processing the Output}

In the body of the lambda, we simply write the received data to \texttt{stdout}
and return the number of bytes we processed (all of them, in this case).


Looking at the original libcurl documentation, you'll see how the Rust bindings are a fairly straightforward translation.

We call \texttt{easy.perform()} to, well, perform the request, blocking until it finishes, and using the callback to process the received data.


\end{frame}


\begin{frame}
\frametitle{M-m-m-multi-cURL}
The real reason we're talking about libcurl is the asynchronous multi interface.

Network communication is a great example of asynchronous I/O.


You can start a network request and move on to creating more without waiting for the results of the first one. 

For requests to different recipients, it certainly makes sense to do this.

\end{frame}



\begin{frame}
\frametitle{The Multi Handle}

The main tool here is the ``multi handle''.

The structure for the new multi-handle type is \texttt{curl::multi::Multi}.

The multi functions may return a \texttt{MultiError} rather than the easy \texttt{Error}.

\end{frame}


\begin{frame}
\frametitle{Pack 'em In}

Once we have a multi handle, we can add easy objects---however many we need---to the multi handle. 

Creation of the easy object is the same as it is when being used alone.

Then, we add the easy (or easy2) object to the multi handle with \texttt{add()} (or \texttt{add2()}). 

The \texttt{add()} or \texttt{add2()} functions return an actual easy handle.
\end{frame}


\begin{frame}
\frametitle{BOMBARDMENT!}

\begin{center}
	\includegraphics[width=0.9\textwidth]{images/bombardment.png}
\end{center}

Once we have finished putting all the easy handles into the multi handle, we can dispatch them all at once with \texttt{perform()}.

\end{frame}


\begin{frame}
\frametitle{Repeat Performance}

This function returns, on success, the number of easy handles in that multi handle that are still running. 

If it's down to 0, then we know that they are all done.

This does mean that we're going to call \texttt{perform()} more than once. 

Doing so doesn't restart or interfere with anything that was already in progress.

We can check often, but the intention is to do other stuff in the meantime.

\end{frame}



\begin{frame}
\frametitle{I'm boooooored}

Suppose we've run out of things to do though. 

We can wait, if we want, using \texttt{wait()}. 

This function will block the current thread until something happens.

The first parameter to \texttt{wait()} is an array of extra file descriptors you can wait on (but we will always want this to be \texttt{\&mut []} in this course).


\end{frame}


\begin{frame}
\frametitle{After Perform}

\begin{center}
	\includegraphics[width=0.6\textwidth]{images/events.jpg}
\end{center}


In the meantime though, the perform operations are happening, and so are whatever callbacks we have set up (if any).

As the I/O operation moves through its life cycle, the state of the easy handle is updated appropriately. 

Each easy handle has an associated status message as well as a return code.

\end{frame}


\begin{frame}
\frametitle{Dude, What Happened?}


We pass \texttt{messages()} a callback which finds out what happened and makes sure all is well.

What we are looking for is that the callback's parameter \texttt{msg} has \texttt{result\_for} including \texttt{Some}---request completed. 

If not, this request is still in progress and we aren't ready to evaluate whether it was successful or not. 

If there are more handles to look at, we should go on to the next. If it is done, we should look at the result. 

If it is \texttt{Error} then there is an error. Else, everything succeeded.

\end{frame}


\begin{frame}
\frametitle{Tidying Up}

When a handle has finished, you need to remove it from the multi handle.

 Remove the handle you got back from \texttt{add/2} with \texttt{remove/2}. 
 
 You don't have to cleanup the easy handle because Rust.


\end{frame}


\begin{frame}[fragile]
\frametitle{Longer Example}

\begin{lstlisting}[language=Rust]
const URLS:[&str; 4] = [
  "https://www.microsoft.com",
  "https://www.yahoo.com",
  "https://www.wikipedia.org",
  "https://slashdot.org" ];

use curl::Error;
use curl::easy::{Easy2, Handler, WriteError};
use curl::multi::{Easy2Handle, Multi};
use std::time::Duration;
use std::io::{stdout, Write};

struct Collector(Vec<u8>);
impl Handler for Collector {
    fn write(&mut self, data: &[u8]) -> Result<usize, WriteError> {
        self.0.extend_from_slice(data);
	stdout().write_all(data).unwrap();
        Ok(data.len())
    }
}
fn init(multi:&Multi, url:&str) -> Result<Easy2Handle<Collector>, Error> {
    let mut easy = Easy2::new(Collector(Vec::new()));
    easy.url(url)?;
    easy.verbose(false)?;
    Ok(multi.add2(easy).unwrap())
}
\end{lstlisting}

\end{frame}


\begin{frame}[fragile]
\frametitle{Longer Example}

\begin{lstlisting}[language=Rust]
fn main() {
    let mut easys : Vec<Easy2Handle<Collector>> = Vec::new();
    let mut multi = Multi::new();
    
    multi.pipelining(true, true).unwrap();
    for u in URLS.iter() {
	easys.push(init(&multi, u).unwrap());
    }
    while multi.perform().unwrap() > 0 {
	// .messages() may have info for us here...
        multi.wait(&mut [], Duration::from_secs(30)).unwrap();
    }

    for eh in easys.drain(..) {
    	let mut handler_after:Easy2<Collector> = multi.remove2(eh).unwrap();
        println!("got response code {}", handler_after.response_code().unwrap());
    }
}
\end{lstlisting}


\end{frame}

\begin{frame}
	\frametitle{Recycle, Reduce, Reuse}

	\begin{center}
		\includegraphics[width=0.3\textwidth]{images/recycle.png}
	\end{center}
	\hfill Image Credit: Wikipedia user Krdan

	Can we re-use an easy handle rather than destroy and create a new one?

\end{frame}

\begin{frame}
	\frametitle{Recycle, Reduce, Reuse}

	The official docs say that you can re-use one.

	But you have to remove it from the multi handle and then re-add it.

	... presumably after having changed anything that you want to change about that handle.

\end{frame}


\begin{frame}
	\frametitle{Always Running}

	You could have a situation where there are constantly handles in progress.

	You might never be at a situation where there are no messages left.

	And that is okay.

\end{frame}

\begin{frame}
	\frametitle{Scaling This}

	The developer claims that you can have multiple thousands of connections in a single multi handle.

	60k ought to be enough for anyone!
\end{frame}


\begin{frame}
	\frametitle{cURL + select}
	\begin{center}
		\includegraphics[width=0.55\textwidth]{images/harold.jpg}
	\end{center}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Process, Threads, AIO?! Four Choices}

  
    \begin{itemize}
      \item Blocking I/O; 1 process per request.
      \item Blocking I/O; 1 thread per request.
      \item Asynchronous I/O, pool of threads, \\callbacks, \\ each thread handles multiple connections.
      \item Nonblocking I/O, pool of threads, \\ multiplexed with select/poll,
        event-driven, \\ each thread handles multiple connections.
    \end{itemize}
  

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Blocking I/O; 1 process per request}

  
  Old Apache model: \hfill \includegraphics[width=.2\textwidth]{images/httpd_logo_wide_new}

  \begin{itemize}
    \item Main thread waits for connections.
    \item Upon connect, forks off a new process, \\which completely
      handles the connection.
    \item Each I/O request is blocking: \\ e.g. reads wait until more data arrives.
  \end{itemize}

  Advantage: 
  \begin{itemize}
    \item ``Simple to understand and easy to program.''
  \end{itemize}

  Disadvantage:
  \begin{itemize}
    \item High overhead from starting 1000s of processes.\\
      (can somewhat mitigate with process pool).
  \end{itemize}
  Can handle $\sim$10 000 processes, but doesn't generally scale.

  

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Blocking I/O; 1 thread per request}

  
    We know that threads are more lightweight than processes.\\[1em]

    Same as 1 process per request, but less overhead.\\[1em]

    I/O is the same---still blocking.\\[1em]

    Advantage:
    \begin{itemize}
      \item Still simple to understand and easy to program.
    \end{itemize}

    Disadvantages:
    \begin{itemize}
      \item Overhead still piles up, although less than processes.
      \item New complication: race conditions on shared data.
    \end{itemize}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Asynchronous I/O Benefits}

  
    In 2006, perf benefits of asynchronous I/O on lighttpd\footnote{\tiny \url{http://blog.lighttpd.net/articles/2006/11/12/lighty-1-5-0-and-linux-aio/}}:\\

{\small
    \begin{tabular}{llrrr}
    version & & fetches/sec & bytes/sec & CPU idle \\
    1.4.13 & sendfile & 36.45 & 3.73e+06 & 16.43\% \\
    1.5.0 & sendfile & 40.51 & 4.14e+06 & 12.77\% \\
    1.5.0 & linux-aio-sendfile & 72.70 & 7.44e+06 & 46.11\% \\
    \end{tabular}
}~\\[1em]

    (Workload: $2\times 7200$ RPM in RAID1, 1GB RAM, \\
     \qquad transferring 10GBytes on a 100MBit network).\\[1em]
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Using Asynchronous I/O in Linux (select/poll)}


   Basic workflow: \\[1em]
   \begin{enumerate}
     \item enqueue a request;
     \item \ldots ~do something else;
     \item (if needed) periodically check whether request is done; and
     \item read the return value.
   \end{enumerate}

See the ECE~252 notes if you want to learn about select/poll!
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}

