\input{configuration}

% Custom packages
\usepackage{xcolor}
\definecolor{solarizedBase03}{HTML}{FFFFFF}
\definecolor{solarizedBase02}{HTML}{000000}
\definecolor{solarizedBase00}{HTML}{657B83}
\definecolor{solarizedBase0}{HTML}{000000}
\definecolor{solarizedBase1}{HTML}{93A1A1}
\definecolor{solarizedBase3}{HTML}{FDF6E3}


\usepackage[listings]{tcolorbox}
\newtcbinputlisting{\codelisting}[2][]{
    listing file={#2},
    colback=solarizedBase03,
    colframe=solarizedBase02,
    colupper=solarizedBase0,
    fonttitle=\bfseries\color{solarizedBase1},
    listing options={basicstyle=\ttfamily\footnotesize},
    listing only,
    #1
}
\newtheorem{defn}{Definition}

\title{Lecture 23 --- Password Cracking, Bitcoin Mining, LLMs }

\author{Patrick Lam \& Jeff Zarnett \\ \small \texttt{patrick.lam@uwaterloo.ca}, \texttt{jzarnett@uwaterloo.ca}}
\institute{Department of Electrical and Computer Engineering \\
  University of Waterloo}
\date{\today}


\begin{document}

\begin{frame}
  \titlepage

 \end{frame}


\part{Password Cracking}

\begin{frame}
\partpage
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Reference: scrypt}

  
    scrypt is the algorithm behind DogeCoin. \\[1em] The reference:\\[1em]
    
    Colin Percival, ``Stronger Key Derivation via Sequential Memory-Hard Functions''. \\
    Presented at BSDCan'09, May~2009.\\[1em]

    \url{http://www.tarsnap.com/scrypt.html}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Acceptable Practices for Password Storage}
  
    \begin{itemize}
    \item {\Huge \alert{{\bf not} plaintext!}}
    \item \Large hashed and salted
    \end{itemize}
       

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Cryptographic hashing}
  
    One-way function:
    \begin{itemize}
    \item $x \mapsto f(x)$ ~~easy to compute; but
      \item $f(x) \stackrel{?~}{\mapsto} x$ ~~hard to reverse.
    \end{itemize}~\\
    Examples: SHA1, scrypt.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Not Secret}
Perhaps passwords have already been leaked!

\begin{center}
	\includegraphics[width=0.75\textwidth]{images/haveibeenpwned.png}
\end{center}


\end{frame}


\begin{frame}
\frametitle{All Too Easy}

The first thing is to try really common passwords.

You just might get a hit!

\begin{center}
	\includegraphics[width=0.7\textwidth]{images/alltooeasy.jpg}
\end{center}

``All too easy.''

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Breaking the Hash}
  
    How can we reverse the hash function?
    \begin{itemize}
    \item    Brute force.
    \end{itemize}~\\
    GPUs (or custom hardware) are good at that!
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{The Arms Race: Making Cracking Difficult}
  
    Historically: force repeated iterations of hashing.\\[1em]

    Main idea behind scrypt (hence DogeCoin):
    \begin{itemize}
    \item make hashing expensive in time and space.
    \end{itemize}~\\
    Implication: require more circuitry to break passwords.\\
    Increases both \# of operations and cost of brute-forcing.
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}
\frametitle{Obligatory xkcd}

Of course, there's always this form of cracking:
\begin{center}
\includegraphics[width=0.75\textwidth]{images/xkcd-538}\\
\hfill (Source: xkcd 538)
\end{center}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Formalizing ``expensive in time and space''}
  
    Want to force use of the ``most memory possible'' for a given
    number of operations.\\[1em]
  
    \begin{defn}
      A \emph{memory-hard} algorithm on a Random Access Machine is an
      algorithm which uses $S(n)$ space and $T(n)$ operations, where
      $S(n) \in \Omega(T(n)^{1-\varepsilon})$.
    \end{defn}~\\
    
      Such algorithms are expensive to implement in either hardware or software.
    

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Generalizing to functions}
  
    Next, add a quantifier: \\move from particular algorithms to underlying functions.\\[1em]
    A \emph{sequential memory-hard function} is one where:

    \begin{itemize}
      \item the fastest
        sequential algorithm is memory-hard; and
      \item it is impossible for a
        parallel algorithm to asymptotically achieve lower cost.
    \end{itemize}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Not made of unicorn tears}
  
    \emph{Exhibit.} ReMix is a concrete example of a sequential memory hard function.\\[1em]
    The scrypt paper concludes with an example of a more realistic (cache-aware) model and
    a function in that context, BlockMix.
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Across the Rainbow Bridge}

If designed well, hash functions aren't easy to brute force.

What if we remembered some previous work? 

It isn't practical, or possible, to store the hashes for every possible plaintext.

\end{frame}


\begin{frame}
\frametitle{Rainbow Table}

The rainbow table is a compromise between speed and space. 

The ``reduction'' function maps hashes to plaintext:

\begin{center}
	\includegraphics[width=0.7\textwidth]{images/rainbow-tables-reduce.png}
\end{center}

Example: \texttt{123456} $\rightarrow$ \texttt{d41d8cd98f00b204e9800998ecf8427e} $\rightarrow$ 418980

\end{frame}


\begin{frame}
\frametitle{Chain Formation}

\begin{center}
	\includegraphics[width=0.7\textwidth]{images/rainbow-tables-reduce.png}
\end{center}

We should do this to develop some number of chains. 

This is the sort of task you could do with a GPU, because they can do a reduction relatively efficiently. 

\end{frame}


\begin{frame}
\frametitle{Or, y'know, just download them}

Once we have those developed for a specific input set and hash function, they can be re-used forever. 

You do not even need to make them yourself anymore (if you don't want) because you can download them on the internet... they are not hard to find. 

They are large, yes, but in the 25--900 GB range. 


\end{frame}


\begin{frame}
\frametitle{Rated `S' for Suck}

\begin{center}
\includegraphics[width=0.3\textwidth]{images/Fallout_76_box_cover.jpg}
\end{center}

I mean, Fallout 76 had a day one patch of 52 GB, and it was a disaster of a game.

\end{frame}


\begin{frame}
\frametitle{How to Use Rainbow Tables}

\begin{enumerate}
	\item Look for the hash in the list of final hashes; if there, we are done
	\item If it's not there, reduce the hash into another plaintext and hash the new plaintext
	\item Go back to step 1
	\item If the hash matches a final hash, the chain with the match contains the original hash
	\item Having identified the correct chain, we can start at the beginning of the chain with the starting plaintext and hash, check to see if we are successful (if so, we are done); if not, reduce and try the next plaintext. 
\end{enumerate} 

\end{frame}


\begin{frame}
\frametitle{Is it Fast?}

Like generation, checking the tables can also be done efficiently by the GPU.

Some numbers from \url{http://www.cryptohaze.com/gpurainbowcracker.php.html}: 

\begin{itemize}
	\item Table generation on a GTX295 core for MD5 proceeds at around 430M links/sec.
	\item Cracking a password 'K\#n\&r4Z': \\ \qquad real: 1m51.962s, user: 1m4.740s. sys: 0m15.320s
\end{itemize}

Yikes.

\end{frame}

\part{Bitcoin Mining}

\begin{frame}
\partpage
\end{frame}

\begin{frame}
\frametitle{The World is Not Enough}

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/bitcoin.png}
\end{center}


\end{frame}


\begin{frame}
\frametitle{Bitcoin Consuption}

\begin{center}
	\includegraphics[width=\textwidth]{images/Bitcoin-usage.png}\\
	\hfill https://digiconomist.net/bitcoin-energy-consumption As of July 2024
\end{center}


\end{frame}

\begin{frame}
\frametitle{Hash Calculations}

Basis: SHA-256

Started with CPU... then GPU... then what?


\end{frame}

\begin{frame}
\frametitle{Custom Hardware}

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/dragonmint.jpeg}
\end{center}

Custom hardware: optimize exactly what you need.

First FPGA, then ASIC...

\end{frame}


\begin{frame}
\frametitle{Please Don't}

We've uncovered why you should not mine Bitcoin.

Not cost efficient; way too much competition.

Not a hardware course, but this is the logical extension of GPU...

\end{frame}

\part{Large Language Models}

\begin{frame}
\partpage
\end{frame}

\begin{frame}
\frametitle{In the Beginning}

In November of 2022, OpenAI introduced ChatGPT to the world...

\begin{center}
	\includegraphics[width=0.4\textwidth]{images/chatgpt.png}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Reaction 1}

\begin{center}
	\includegraphics[width=\textwidth]{images/skynet.png}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Reaction 2}

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/startup.jpg}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Reaction 3}

\begin{center}
	\includegraphics[width=0.75\textwidth]{images/prompt.jpg}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Reaction 4}

\begin{center}
	\includegraphics[width=0.75\textwidth]{images/starterpack.jpg}
\end{center}

\end{frame}


\begin{frame}
\frametitle{ChatGPT}

Such large language models have existed before, but ChatGPT ended up a hit because it's pretty good at being ``conversational''.

This is within the ambit of Natural Language Processing (NLP).

\end{frame}

\begin{frame}
\frametitle{ChatGPT}

Just because it gives you an answer, doesn't mean the answer is correct or true.

\begin{center}
	\includegraphics[width=0.4\textwidth]{images/le-failed.jpg}
\end{center}

AI is subject to \alert{hallucinations}.

\end{frame}

\begin{frame}
\frametitle{Remember, The Law...}
Legally and professionally, the engineer is responsible for understanding how a given tool works \& verifying the output is reasonable \& correct.

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/jail.jpg}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Prepared in Advance?}
Part of what makes the GPT-3 and GPT-4 models better at producing output that matches our expectations is that it relies on pre-training.

This course is not one on neural networks, large language models, AI, or similar.

But performance is relevant here!

\end{frame}

\begin{frame}
\frametitle{Parameters}

One factor that matters in how good a model is: parameters.

Bigger is \textit{usually} better...\\
\quad But requires more computational and memory resources.

We can maybe tune some options!

\end{frame}

\begin{frame}
\frametitle{Let's Try Some Optimization}

This section is based on a guide from ``Hugging Face''. 

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/hugging-face.png}
\end{center}

You may have guessed by the placement of this topic in the course material that the GPU is the right choice for how to generate or train a large language model. 

\end{frame}

\begin{frame}
\frametitle{About that Name}

Just... uh... don't confuse Hugging Face with Facehugger...

\begin{center}
	\includegraphics[width=0.75\textwidth]{images/ripley.jpg}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Why a GPU?}

In this case we're talking about Transformers.

There are three main groups of optimizations that it does:\\
\begin{itemize} 
	\item Tensor Contractions
	\item Statistical Normalizations
	\item Element-Wise Operators
\end{itemize}

We also need to consider what's in memory.

\end{frame}

\begin{frame}
\frametitle{Optimize What?}

We can focus on how to generate a model that gives answers quickly...

Or we can focus on how to generate or train the model quickly.

\end{frame}

\begin{frame}
\frametitle{Make a Fast Model}

Use more space to reduce CPU usage, optimize for common cases, speculate...

Some of these are more fun than others: given a particular question, can you guess what the followup might be? 

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/predictable.jpg}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Make a Model Fast}

Why would we customize some LLM?

Don't send your data to OpenAI...

Specialize for your workload.

\end{frame}

\begin{frame}
\frametitle{Batch Size}

Our first major optimization, and perhaps the easiest to do, is the batch size.

\begin{center}
	\includegraphics[width=0.3\textwidth]{images/badbatch.jpg}
\end{center}


\end{frame}

\begin{frame}
\frametitle{Let's Review the Code}

The code is a little too large for the slides so let's go over it in a code editor.


The bert-large-uncased model is about 340~MB.

It's uncased because it makes no distinction between capitals and lower-case letters, e.g., it sees ``Word'' and ``word'' as equivalent.

\end{frame}

\begin{frame}[fragile]
\frametitle{Let's Try Training}

{\scriptsize
\begin{verbatim}
jzarnett@ecetesla0:~/github/ece459/lectures/live-coding/L24$ python3 dummy_data.py 
Starting up. Initial GPU utilization:
GPU memory occupied: 0 MB.
Initialized Torch; current GPU utilization:
GPU memory occupied: 417 MB.
Some weights of BertForSequenceClassification were not initialized 
from the model checkpoint at  bert-large-uncased and are newly initialized: 
['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able 
to use it for predictions and inference.
GPU memory occupied: 1705 MB.
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 
7.43 GiB total capacity; 6.90 GiB already allocated; 16.81 MiB free; 6.90 GiB 
reserved in total by PyTorch) If reserved memory is >> allocated memory try setting 
max_split_size_mb to avoid fragmentation.  See documentation for Memory Management
and PYTORCH_CUDA_ALLOC_CONF
\end{verbatim}
}

\end{frame}

\begin{frame}[fragile]
\frametitle{Let's See the Problem}
I asked \texttt{nvidia-smi}...

{\scriptsize
\begin{verbatim}
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla P4            Off  | 00000000:17:00.0 Off |                    0 |
| N/A   42C    P0    23W /  75W |      0MiB /  7611MiB |      1%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
\end{verbatim}
}


\end{frame}

\begin{frame}
\frametitle{Kirk to Engineering...}

7~611 MB is not enough for this model...

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/morepower.png}
\end{center}

Problem is we don't have any bigger VRAM cards.

\end{frame}

\begin{frame}
\frametitle{Fine, I Guess}

What I actually did next was change to a smaller version of the model, \texttt{bert-base-uncased}.

It's significantly smaller (110~MB) and something the card could handle. 

\end{frame}

\begin{frame}[fragile]
\frametitle{Smaller Model Output}

{\scriptsize
\begin{verbatim}
jzarnett@ecetesla0:~/github/ece459/lectures/live-coding/L24$ python3 dummy_data.py 
Starting up. Initial GPU utilization:
GPU memory occupied: 0 MB.
Initialized Torch; current GPU utilization:
GPU memory occupied: 417 MB.
Some weights of BertForSequenceClassification were not initialized from the 
model checkpoint at  bert-base-uncased and are newly initialized: 
['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use
 it for predictions and inference.
GPU memory occupied: 887 MB.
{'loss': 0.0028, 'learning_rate': 1.1718750000000001e-06, 'epoch': 0.98}
{'train_runtime': 109.6152, 'train_samples_per_second': 4.671, 
'train_steps_per_second': 4.671, 'train_loss': 0.0027378778694355788, 'epoch': 1.0}
Time: 109.62
Samples/second: 4.67
GPU memory occupied: 3281 MB.
\end{verbatim}
}

\end{frame}

\begin{frame}
\frametitle{Results Table}

Then I needed to experiment some more with batch size to find the ideal.

\begin{center}
\begin{tabular}{r|r|r|r|r}
\textbf{Batch Size} & \textbf{Time (s)} & \textbf{Samples/s} & \textbf{Memory Used (MB)} & \textbf{Utilization (\%)} \\ \hline
1 & 109.62 & 4.67 & 3~281 & 43.1 \\
2 & 85.82 & 5.97 & 3~391 & 44.6 \\
4 & 72.18 & 7.09 & 4~613 & 60.6 \\
8 & 66.70 & 7.68 & 7~069 & 92.9 \\
\end{tabular}
\end{center}

Unsurprisingly, choosing batch size of 9 leads to OOM.

\end{frame}

\begin{frame}
\frametitle{Next Idea}

We seem to be memory limited---let's see what we can do?

First idea: \alert{gradient accumulation}. 

Calculate gradients in small increments rather than for the whole batch.

\end{frame}

\begin{frame}
\frametitle{Experimenting with Gradient Accumulation}

\begin{center}
\begin{tabular}{r|r|r|r|r}
\textbf{Steps} & \textbf{Time (s)} & \textbf{Samples/s} & \textbf{Memory Used (MB)} & \textbf{Utilization (\%)} \\ \hline
1 & 66.06 & 7.75 & 7~069 & 92.9 \\
2 & 63.96 & 8.01 & 7~509 & 98.7 \\
4 & 62.81 & 8.15 & 7~509 & 98.7 \\
8 & 62.65 & 8.17 & 7~509 & 98.7 \\
16 & 62.42 & 8.20 & 7~509 & 98.7 \\
32 & 62.44 & 8.20 & 7~509 & 98.7\\
128 & 62.20 & 8.23 & 6~637 & 87.2\\
1024 & 61.78 & 8.29 & 6~637 & 87.2 \\
4096 & 62.16 & 8.24 & 6~637 & 87.2
\end{tabular}
\end{center}

\end{frame}

\begin{frame}
\frametitle{I got Suspicious}

I got suspicious about the 128 dropoff in memory usage and it made me think about other indicators---is it getting worse somehow? 

The output talks about training loss...

\begin{center}
\begin{tabular}{r|r}
\textbf{Gradient Accumulation Steps} & \textbf{Loss} \\ \hline
1 & 0.029 \\
2 & 0.070 \\
4 & 0.163 \\
8 & 0.169 \\
16 & 0.447 \\
32 & 0.445 \\
128 & 0.435 \\
1024 & 0.463 \\
4096 & 0.014 \\
\end{tabular}
\end{center}

Is that concerning?

\end{frame}

\begin{frame}
\frametitle{Let's Validate}

We need to do some validation of how it's going...


We'll train and validate using some Yelp data.

\begin{center}
	\includegraphics[width=0.6\textwidth]{images/yelp.png}
\end{center}

\end{frame}

\begin{frame}
\frametitle{More Code, More Review}

The python code here is significantly different but we'll take a look.

I've skipped some intermediate results since at 9 minutes to calculate it takes a while to fill in all the values above.


\end{frame}

\begin{frame}
\frametitle{Accuracy Results}

\begin{center}
\begin{tabular}{r|r|r|r|r}
\textbf{GA Steps} & \textbf{Time (s)} & \textbf{Samples/s} & \textbf{Memory Used (MB)} & \textbf{Final Accuracy}\\ \hline
1 & 538.37 & 5.56 & 7~069 & 0.621 \\
8 & 501.89 & 5.98 & 7~509 & 0.554 \\
32 & 429.70 & 6.98 & 7~509 & 0.347 \\
1024 & 513.17 & 5.85 & 7~509 & 0.222 \\
\end{tabular}
\end{center}

Interesting results---maybe a little concerning?

\end{frame}

\begin{frame}
\frametitle{More Isn't Always Better}

Increasing the gradient accumulation does change the effective batch size...

Batch size too large may mean less ability to generalize (overfitting).

But smaller isn't always better either; can underfit the data.

In the Yelp example, I get worse accuracy with batch size of 1 than 4, and 4 is worse than 8. There really is no magic number.


\end{frame}

\begin{frame}
\frametitle{Gradient Checkpointing}

\alert{Gradient Checkpointing}: increase compute time to save memory.

Instead of saving activations, save only some, recompute the rest.

Does it work?

\end{frame}

\begin{frame}
\frametitle{Gradient Checkpointing}

Trying this out with batch size of 8, the total time goes from 66.70 to 93.07s and the memory from 7~069 down to 3~619 MB. 

As expected, we got slower but used less memory. 

Maybe it means we can increase the batch size? 

Raising it to 16 means the time was  100.55s but still only 3~731 MB


\end{frame}

\begin{frame}
\frametitle{Gradient Checkpointing}

Increasing the batch size a lot to finish faster might work eventually. 

And no, using this checkpointing even with a batch size of 1 is not sufficient to run the \texttt{bert-large-uncased} model on \texttt{ecetesla0}.

And remember that excessively large batch sizes aren't good...

\end{frame}

\begin{frame}
\frametitle{Mixed Precision, Data Preloading}

\alert{Mixed Precision}: Use less accurate types (16 vs 32-bit).

\alert{Data Preloading}: Multithread to get data to GPU faster.

Other ideas: Mixture of Experts, bigger GPU, multiple-GPU...

\end{frame}

\begin{frame}
\frametitle{Tradeoffs}
\begin{multicols}{2}
\includegraphics[width=0.35\textwidth]{images/tradeoffer.jpg}
\columnbreak
\begin{itemize}
\item Accuracy for time? \\[5em]

\item Memory for CPU? \\[5em]

\item Err on the side of over- or under-fitting?
\end{itemize}
\end{multicols}

\end{frame}

\begin{frame}
\frametitle{Tradeoffs}

In the next few years, technologies and decision-making for this will become much more sophisticated.

But in the meantime, we can have a lot of fun experimenting and learning.
\begin{center}
	\includegraphics[width=0.4\textwidth]{images/funbegins.jpg}
\end{center}
\end{frame}


\end{document}

